Close-kin pairwise probability formulae are usually quite simple,
at least with hindsight, but they still can be awkard to get right
in the first place. One way to reduce the risk of mistakes is to generate
simulated datasets, and check that the CKMR code is giving the expected
results when known parameter values are inserted. CKMR simulation
code looks utterly different from kinship-probability code, and the
chance of ``making the same mistake twice'' is therefore much less
than with many statistical simulations. Robustness is improved even
further if two different people are involved, one to simulate and
one to write kinship-probability code. Even though simulation is not
strictly necessary for most CKMR design exercises, simulation may
be worth the additional effort in order to help the whole process,
and that is the approach we took for walrus. We did find and fix several
mistakes this way, both in the CKMR code and in the simulation code,
so the exercise was certainly worthwhile.

The obvious question is how to approach CKMR model-checking when simulated
datasets are available. There are various options and no . One thing
to avoid, if possible, is the naive and laborious approach of actually
\emph{fitting} a CKMR to each simulated dataset, which can be painfully
slow. %
\begin{lyxgreyedout}
(Note, perhaps for discussion: We started this project before RTMB
became available, expecting that the actual model-fitting code for
real data would eventually have to be written in TMB itself, but keen
to avoid the complexity of TMB at the design stage. In contrast, design
calculations are quick because it is only necessary to calculate probability
arrays once, and R alone is adequately fast, without TMB or RTMB.
However, it would not be practical to fit even our simple model to
multiple datasets without RTMB; and even with RTMB, repeated fitting
of a more complicted model, e.g. with copious random effects, might
be a challenge.)%
\end{lyxgreyedout}
{} We used several checks. All are aimed at detecting gross errors (and
we did find some); power to detect subtle mistakes is lower, but in
our experience subtle mistakes are actually less likely than big ones.
The first two checks are based on single realizations of simulated
data, and so are also suitable as diagnostics when fitting to real
data; the last two require multiple simulated datasets.

\begin{comment}
One option is to include final result(s) for each check right here,
after describing the check. Otherwise, if this section goes mainly
into Methods, the results of each little check will be a long way
from the text describing it, and the reader will have forgotten what
the check is by the time they see its results. I absolutely hate papers
like that :) But, it might depend; needs Zoom.
\end{comment}

\begin{itemize}
\item Observed and expected totals of sampled kin-pairs of each type. Clearly,
unless these match reasonably well, there must be a major inconsistency
between model and simulationg. The definition of ``reasonably well''
can be guided by the inherent Poisson variability. If an expected
total is 227, say, then we would not expect to see observed total
much outside, say, the 95\% confidence limits for a Poisson distribution
with mean (and therefore variance) 227. This can be roughly approximated
by $227\pm2\sqrt{227}$ or about {[}195,255{]}. Clearly, the expected
total needs to be fairly large for this to have much power, so it
might be useful to increase the simulated sample size for checking
purposes.\\
{*}{*}OPTION{*}{*}list the totals here (for first test dataset, chosen
so that sim matches CK code as closely as possible)
\item Breakdown of observed and expected kin-pair totals across some covariate
of interest. If the totals from the previous step are not matching
well, then the breakdown may shed light on where to look for problems.
For example: the distribution of birth-gaps between XmHSPs is driven
in the longer term by the adult rate mortality rate, so if observed
and expected do not correspond, then the treatment of mortality is
likely inconsistent. Also, the number of mothers by age-at-birth should
fluctuate over the first few years of adulthood because of the typically-three-year
breeding cycle (most 6yo have just given birth; most 7yo are still
nursing last year's offspring, etc), until it settles down because
of the averaging effects of irregularities. If the observed and expected
patterns do not match, then the breeding cycle treatment is inconsistent.\\
{*}{*}OPTION{*}{*} show the 2 graphs here.
\item P-values of observed kin-totals by type, based on the Poisson distribution
as above. Given a reasonable number of simulated datasets (say 20
or more), these should be roughly uniform across the interval {[}0,1{]}.
Clearly, it would require a large number of simulations to get a precise
check here, but precision is not necessary: the goal is to pick up
fairly coarse errors.\\
{*}{*}OPTION{*}{*} show 4 histos here (instead of box'n'whiska)
\item Looking at the mean and variance of the derivative of the pseudo-log-likelihood
at the true parameter values $\theta_{0}$ (something which can be
calculated fairly quickly by numerical differentiation). The mean
should be close to 0 and the variance determines what ``close''
might mean, given the number of simulations available. This checks
the crucial ``unbiased estimating equation'' (UEE) assumption required
by most statistical estimation frameworks, including maximum-likelihood.
If UEE does not hold, then by definition there is a mismatch between
simulation and model.\\
{*}{*}OPTION{*}{*} there's some numbers printeed at the end of compare2sims.R,
I thnk.
\end{itemize}
The description so far implicitly assumes that the CKMR model (if
working right) corresponds exactly to the data-generation mechanism
in the simulations. However, it might be desirable to make the CKMR
model simpler, especially for design purposes where the goal is just
to make sure that sampling plans are sensible; developing a more complicated
and realistic model can often be left until the real data appears.
For example, we wanted to avoid reproductive senescence in the CKMR
equations, so that all adults could be treated as a single block without
requiring age-structured dynamics inside the model. Nevertheless,
senescence is likely a reality of the walrus world, and there is such
a thing as ``too simple to be useful'', so it is worth checking
whether the simpler formulation is going to run into serious trouble.
Simulated datasets can be used to estimate approximate bias in a slightly-mis-specified
CKMR model, again without needing to do any estimation. The idea is
to approximate the MLE for each dataset, based only on calculations
using the true parameter value for the simulations. The MLE $\hat{\theta}$
will by definition satisfy the equation $\left.d\Lambda/d\theta\right\vert _{\hat{\theta}}=0$,
and we can take a first-order Taylor expansion around the true value
$\theta_{0}$ to give

\begin{gather}
0=\left.\frac{d\Lambda}{d\theta}\right\vert _{\hat{\theta}}\approx\left.\frac{d\Lambda}{d\theta}\right\vert _{\theta_{0}}+\left(\hat{\theta}-\theta_{0}\right)\left.\frac{d^{2}\Lambda}{d\theta^{2}}\right\vert _{\theta_{0}}\nonumber \\
\implies\hat{\theta}-\theta_{0}\approx-\left[\left.\frac{d\Lambda^{2}}{d\theta^{2}}\right\vert _{\theta_{0}}\right]^{-1}\left.\frac{d\Lambda}{d\theta}\right\vert _{\theta_{0}}\label{eq:bias-approx}
\end{gather}

The square-bracketed term can be replaced (to the same order of accuracy
as the rest of the approxmation) by the \emph{expected} Hessian which
is the crux of our design calculations anyway, and which of course
does not vary from one simulation to the next. Thus, the only quantity
that has to be calculated per simulated dataset is $\left.d\Lambda/d\theta\right\vert _{\theta_{0}}$,
already required for the unbiased-estimating-equation check above.
The estimated bias is the average across simulations of (\ref{eq:bias-approx}).
This is quite similar to the UEE check above, but with a change in
focus: this time, we may be prepared to tolerate some small violation
of UEE, provided that it does not imply substantial bias on the parameter
scale. In particular, if the estimated bias for the $r$\textsuperscript{th}
parameter (i.e. $r$\textsuperscript{th } component of $\theta$)
is below its sampling variability— say, if bias is less than 1 standard
deviation, computed from the square-root of the diagonal of the inverse
Hessian or $\sqrt{H^{-1}\left(r,r\right)}$— then there is little
reason to worry about bias for that particular parameter. 

{*}{*}OPTION{*}{*} stuff from the end of compare2sims.R

{*}{*}DISCUSSION?{*}{*}

In the end, based on the checks above, our estimation and simulation
codes did indeed appear consistent, and any bias induced by (among
other minor things) ignoring senescence did not seem problematic.
Of course, we only reached that position \emph{after} going thru the
checking process several times, to find and fix inconsistencies.
